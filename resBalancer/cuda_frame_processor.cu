#include "cuda_frame_processor.h"
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <iostream>
#include <memory>
#include <algorithm>

// CUDA kernel for high-performance bilinear resize
__global__ void cuda_resize_bilinear_kernel(
    const unsigned char* __restrict__ input,
    unsigned char* __restrict__ output,
    int input_width, int input_height,
    int output_width, int output_height,
    int channels, size_t input_pitch, size_t output_pitch) {
    
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (x >= output_width || y >= output_height) return;
    
    // Calculate the corresponding position in the input image
    float src_x = (float)x * input_width / output_width;
    float src_y = (float)y * input_height / output_height;
    
    int x1 = (int)src_x;
    int y1 = (int)src_y;
    int x2 = min(x1 + 1, input_width - 1);
    int y2 = min(y1 + 1, input_height - 1);
    
    float fx = src_x - x1;
    float fy = src_y - y1;
    
    // Bilinear interpolation for each channel
    for (int c = 0; c < channels; c++) {
        float val = (1.0f - fx) * (1.0f - fy) * input[(y1 * input_pitch) + x1 * channels + c] +
                    fx * (1.0f - fy) * input[(y1 * input_pitch) + x2 * channels + c] +
                    (1.0f - fx) * fy * input[(y2 * input_pitch) + x1 * channels + c] +
                    fx * fy * input[(y2 * input_pitch) + x2 * channels + c];
        
        output[(y * output_pitch) + x * channels + c] = (unsigned char)__float2int_rn(val);
    }
}

// CUDA kernel for horizontal flip (mirror)
__global__ void cuda_mirror_horizontal_kernel(
    const unsigned char* __restrict__ input,
    unsigned char* __restrict__ output,
    int width, int height, int channels,
    size_t input_pitch, size_t output_pitch) {
    
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (x >= width || y >= height) return;
    
    int flipped_x = width - 1 - x;
    
    // Copy pixels with horizontal flip
    for (int c = 0; c < channels; c++) {
        output[(y * output_pitch) + flipped_x * channels + c] = 
            input[(y * input_pitch) + x * channels + c];
    }
}

// CUDA kernel for Gaussian blur (performance optimization)
__global__ void cuda_gaussian_blur_kernel(
    const unsigned char* __restrict__ input,
    unsigned char* __restrict__ output,
    int width, int height, int channels,
    size_t pitch, float sigma) {
    
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (x >= width || y >= height) return;
    
    // 5x5 Gaussian kernel for sigma ~= 1.0
    const float kernel[5][5] = {
        {0.003765f, 0.015019f, 0.023792f, 0.015019f, 0.003765f},
        {0.015019f, 0.059912f, 0.094907f, 0.059912f, 0.015019f},
        {0.023792f, 0.094907f, 0.150342f, 0.094907f, 0.023792f},
        {0.015019f, 0.059912f, 0.094907f, 0.059912f, 0.015019f},
        {0.003765f, 0.015019f, 0.023792f, 0.015019f, 0.003765f}
    };
    
    for (int c = 0; c < channels; c++) {
        float sum = 0.0f;
        
        for (int ky = -2; ky <= 2; ky++) {
            for (int kx = -2; kx <= 2; kx++) {
                int px = min(max(x + kx, 0), width - 1);
                int py = min(max(y + ky, 0), height - 1);
                
                sum += kernel[ky + 2][kx + 2] * input[(py * pitch) + px * channels + c];
            }
        }
        
        output[(y * pitch) + x * channels + c] = (unsigned char)__float2int_rn(sum);
    }
}

// CUDA error checking macro
#define CUDA_CHECK(call) \
    do { \
        cudaError_t error = call; \
        if (error != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ \
                      << " - " << cudaGetErrorString(error) << std::endl; \
            return false; \
        } \
    } while(0)

#define CUDA_CHECK_VOID(call) \
    do { \
        cudaError_t error = call; \
        if (error != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ \
                      << " - " << cudaGetErrorString(error) << std::endl; \
            return; \
        } \
    } while(0)

// CudaFrameProcessor Implementation
CudaFrameProcessor::CudaFrameProcessor(const CudaConfig& cfg) 
    : config(cfg), cuda_context(nullptr), memory_pool(nullptr), 
      memory_pool_size(0), processing_times(nullptr), time_index(0), initialized(false) {
    
    // Initialize CUDA streams array
    for (int i = 0; i < 8; i++) {
        cuda_streams[i] = nullptr;
    }
    
    // Reserve space for frame buffers
    frame_buffers.reserve(10);  // Reserve space for efficiency
}

CudaFrameProcessor::~CudaFrameProcessor() {
    shutdown();
}

bool CudaFrameProcessor::initialize() {
    // Check CUDA device availability
    int device_count = 0;
    CUDA_CHECK(cudaGetDeviceCount(&device_count));
    
    if (device_count == 0) {
        std::cerr << "No CUDA devices found" << std::endl;
        return false;
    }
    
    // Set device
    if (config.device_id >= device_count) {
        std::cerr << "Invalid device ID: " << config.device_id << std::endl;
        return false;
    }
    
    CUDA_CHECK(cudaSetDevice(config.device_id));
    
    // Get device properties for safety checks
    cudaDeviceProp prop;
    CUDA_CHECK(cudaGetDeviceProperties(&prop, config.device_id));
    
    std::cout << "Initializing CUDA on device: " << prop.name << std::endl;
    std::cout << "Compute capability: " << prop.major << "." << prop.minor << std::endl;
    std::cout << "Global memory: " << prop.totalGlobalMem / (1024*1024) << " MB" << std::endl;
    
    // Validate memory requirements
    size_t required_memory = config.memory_pool_size_mb * 1024 * 1024;
    if (required_memory > prop.totalGlobalMem * 0.8) { // Use max 80% of GPU memory
        std::cerr << "Requested memory pool too large for device" << std::endl;
        return false;
    }
    
    // Create CUDA streams for concurrent processing
    if (!create_cuda_streams()) {
        std::cerr << "Failed to create CUDA streams" << std::endl;
        return false;
    }
    
    // Allocate memory pools
    if (!allocate_memory_pools()) {
        std::cerr << "Failed to allocate memory pools" << std::endl;
        destroy_cuda_streams();
        return false;
    }
    
    // Allocate performance tracking array
    processing_times = new float[100];
    memset(processing_times, 0, sizeof(float) * 100);
    
    // Enable memory pinning if requested
    if (config.enable_memory_pinning) {
        // This would be implemented for zero-copy transfers
        std::cout << "Memory pinning enabled for zero-copy transfers" << std::endl;
    }
    
    initialized = true;
    std::cout << "âœ… CUDA frame processor initialized successfully" << std::endl;
    return true;
}

void CudaFrameProcessor::shutdown() {
    if (!initialized) return;
    
    // Synchronize all streams before cleanup
    for (int i = 0; i < config.max_concurrent_streams; i++) {
        if (cuda_streams[i]) {
            cudaStreamSynchronize((cudaStream_t)cuda_streams[i]);
        }
    }
    
    // Clean up resources
    cleanup_memory_pools();
    destroy_cuda_streams();
    
    if (processing_times) {
        delete[] processing_times;
        processing_times = nullptr;
    }
    
    // Reset CUDA device
    cudaDeviceReset();
    
    initialized = false;
    std::cout << "ðŸ›‘ CUDA frame processor shutdown complete" << std::endl;
}

bool CudaFrameProcessor::process_frame_async(const unsigned char* input, int width, int height,
                                           unsigned char* output, int output_width, int output_height,
                                           int stream_id) {
    if (!initialized || stream_id >= config.max_concurrent_streams) {
        return false;
    }
    
    // Safety checks
    if (!input || !output || width <= 0 || height <= 0 || 
        output_width <= 0 || output_height <= 0) {
        return false;
    }
    
    // Dimension safety checks (prevent OpenCV assertion failures)
    if (width > 32767 || height > 32767 || output_width > 32767 || output_height > 32767) {
        std::cerr << "Frame dimensions exceed safe limits" << std::endl;
        return false;
    }
    
    cudaStream_t stream = (cudaStream_t)cuda_streams[stream_id];
    
    // Get appropriate frame buffer
    CudaFrameBuffer* buffer = allocate_frame_buffer(width, height, 3);
    if (!buffer) {
        return false;
    }
    
    // Calculate memory requirements
    size_t input_size = width * height * 3;
    size_t output_size = output_width * output_height * 3;
    
    // Copy input to GPU (async)
    CUDA_CHECK(cudaMemcpyAsync(buffer->d_input, input, input_size, 
                              cudaMemcpyHostToDevice, stream));
    
    // Configure kernel launch parameters
    dim3 block_size(16, 16);
    dim3 grid_size((output_width + block_size.x - 1) / block_size.x,
                   (output_height + block_size.y - 1) / block_size.y);
    
    // Launch resize kernel
    cuda_resize_bilinear_kernel<<<grid_size, block_size, 0, stream>>>(
        (const unsigned char*)buffer->d_input,
        (unsigned char*)buffer->d_output,
        width, height, output_width, output_height, 3,
        buffer->pitch, buffer->pitch
    );
    
    // Check for kernel launch errors
    CUDA_CHECK(cudaGetLastError());
    
    // Copy result back to host (async)
    CUDA_CHECK(cudaMemcpyAsync(output, buffer->d_output, output_size,
                              cudaMemcpyDeviceToHost, stream));
    
    return true;
}

bool CudaFrameProcessor::wait_for_completion(int stream_id) {
    if (!initialized || stream_id >= config.max_concurrent_streams) {
        return false;
    }
    
    cudaStream_t stream = (cudaStream_t)cuda_streams[stream_id];
    CUDA_CHECK(cudaStreamSynchronize(stream));
    return true;
}

bool CudaFrameProcessor::process_frame_batch(unsigned char** inputs, int batch_size,
                                           int width, int height, unsigned char** outputs,
                                           int output_width, int output_height) {
    if (!initialized || batch_size <= 0) {
        return false;
    }
    
    // Process each frame in the batch using different streams
    for (int i = 0; i < batch_size; i++) {
        int stream_id = i % config.max_concurrent_streams;
        
        if (!process_frame_async(inputs[i], width, height, outputs[i], 
                               output_width, output_height, stream_id)) {
            return false;
        }
    }
    
    // Wait for all streams to complete
    for (int i = 0; i < config.max_concurrent_streams; i++) {
        wait_for_completion(i);
    }
    
    return true;
}

CudaFrameBuffer* CudaFrameProcessor::allocate_frame_buffer(int width, int height, int channels) {
    // Find available buffer or create new one
    for (auto& buffer : frame_buffers) {
        if (buffer.width == width && buffer.height == height && buffer.channels == channels) {
            return &buffer;
        }
    }
    
    // Create new buffer
    CudaFrameBuffer buffer;
    buffer.width = width;
    buffer.height = height;
    buffer.channels = channels;
    
    // Calculate pitched memory requirements
    size_t width_bytes = width * channels;
    CUDA_CHECK(cudaMallocPitch(&buffer.d_input, &buffer.pitch, width_bytes, height));
    CUDA_CHECK(cudaMallocPitch(&buffer.d_output, &buffer.pitch, width_bytes, height));
    CUDA_CHECK(cudaMallocPitch(&buffer.d_temp, &buffer.pitch, width_bytes, height));
    
    frame_buffers.push_back(buffer);
    return &frame_buffers.back();
}

void CudaFrameProcessor::release_frame_buffer(CudaFrameBuffer* buffer) {
    if (!buffer) return;
    
    // In a real implementation, this would return buffer to pool
    // For now, buffers are cleaned up in shutdown()
}

float CudaFrameProcessor::get_average_processing_time() const {
    if (!processing_times) return 0.0f;
    
    float sum = 0.0f;
    int count = 0;
    
    for (int i = 0; i < 100; i++) {
        if (processing_times[i] > 0.0f) {
            sum += processing_times[i];
            count++;
        }
    }
    
    return count > 0 ? sum / count : 0.0f;
}

float CudaFrameProcessor::get_gpu_utilization() const {
    // This would use NVML to get actual GPU utilization
    // For now, return estimated utilization based on processing times
    float avg_time = get_average_processing_time();
    float target_time = 1000.0f / 30.0f; // 30 FPS target
    
    return std::min(100.0f, (avg_time / target_time) * 100.0f);
}

size_t CudaFrameProcessor::get_memory_usage() const {
    size_t total_usage = 0;
    
    for (const auto& buffer : frame_buffers) {
        size_t buffer_size = buffer.pitch * buffer.height * 3; // 3 buffers per frame buffer
        total_usage += buffer_size;
    }
    
    return total_usage;
}

bool CudaFrameProcessor::allocate_memory_pools() {
    // Allocate main memory pool
    memory_pool_size = config.memory_pool_size_mb * 1024 * 1024;
    
    CUDA_CHECK(cudaMalloc(&memory_pool, memory_pool_size));
    
    std::cout << "Allocated CUDA memory pool: " << config.memory_pool_size_mb << " MB" << std::endl;
    return true;
}

void CudaFrameProcessor::cleanup_memory_pools() {
    // Free all frame buffers
    for (auto& buffer : frame_buffers) {
        if (buffer.d_input) cudaFree(buffer.d_input);
        if (buffer.d_output) cudaFree(buffer.d_output);
        if (buffer.d_temp) cudaFree(buffer.d_temp);
    }
    frame_buffers.clear();
    
    // Free main memory pool
    if (memory_pool) {
        cudaFree(memory_pool);
        memory_pool = nullptr;
    }
}

bool CudaFrameProcessor::create_cuda_streams() {
    for (int i = 0; i < config.max_concurrent_streams && i < 8; i++) {
        cudaStream_t stream;
        CUDA_CHECK(cudaStreamCreate(&stream));
        cuda_streams[i] = (void*)stream;
    }
    
    std::cout << "Created " << config.max_concurrent_streams << " CUDA streams" << std::endl;
    return true;
}

void CudaFrameProcessor::destroy_cuda_streams() {
    for (int i = 0; i < 8; i++) {
        if (cuda_streams[i]) {
            cudaStreamDestroy((cudaStream_t)cuda_streams[i]);
            cuda_streams[i] = nullptr;
        }
    }
}

void CudaFrameProcessor::cleanup_cuda_streams() {
    // Alias for destroy_cuda_streams for compatibility
    destroy_cuda_streams();
}

// C API Implementation
extern "C" {
    DLLEXPORT CudaFrameProcessor* create_cuda_processor(CudaConfig config) {
        CudaFrameProcessor* processor = new CudaFrameProcessor(config);
        if (!processor->initialize()) {
            delete processor;
            return nullptr;
        }
        return processor;
    }
    
    DLLEXPORT void destroy_cuda_processor(CudaFrameProcessor* processor) {
        if (processor) {
            delete processor;
        }
    }
    
    DLLEXPORT int cuda_process_frame(CudaFrameProcessor* processor,
                                   unsigned char* input, int width, int height,
                                   unsigned char* output, int output_width, int output_height) {
        if (!processor) return 0;
        
        if (processor->process_frame_async(input, width, height, output, output_width, output_height, 0)) {
            return processor->wait_for_completion(0) ? 1 : 0;
        }
        return 0;
    }
    
    DLLEXPORT int cuda_process_frame_batch(CudaFrameProcessor* processor,
                                         unsigned char** inputs, int batch_size,
                                         int width, int height,
                                         unsigned char** outputs,
                                         int output_width, int output_height) {
        if (!processor) return 0;
        
        return processor->process_frame_batch(inputs, batch_size, width, height, 
                                            outputs, output_width, output_height) ? 1 : 0;
    }
    
    DLLEXPORT float cuda_get_processing_time(CudaFrameProcessor* processor) {
        return processor ? processor->get_average_processing_time() : 0.0f;
    }
    
    DLLEXPORT float cuda_get_gpu_utilization(CudaFrameProcessor* processor) {
        return processor ? processor->get_gpu_utilization() : 0.0f;
    }
    
    DLLEXPORT int cuda_get_memory_usage_mb(CudaFrameProcessor* processor) {
        if (!processor) return 0;
        return (int)(processor->get_memory_usage() / (1024 * 1024));
    }
    
    DLLEXPORT int cuda_get_device_count() {
        int count = 0;
        cudaGetDeviceCount(&count);
        return count;
    }
    
    DLLEXPORT int cuda_get_device_memory_mb(int device_id) {
        cudaDeviceProp prop;
        if (cudaGetDeviceProperties(&prop, device_id) == cudaSuccess) {
            return (int)(prop.totalGlobalMem / (1024 * 1024));
        }
        return 0;
    }
    
    DLLEXPORT int cuda_set_device(int device_id) {
        return cudaSetDevice(device_id) == cudaSuccess ? 1 : 0;
    }
    
    DLLEXPORT int cuda_is_available() {
        int count = 0;
        return cudaGetDeviceCount(&count) == cudaSuccess && count > 0 ? 1 : 0;
    }
}
